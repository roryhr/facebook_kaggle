{"cells":[
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "# Summary\n\nWe're given the prompt from Facebook: \n> The goal of this competition is to predict which place a person would like to check in to. \nFor the purposes of this competition, Facebook created an artificial world consisting of more than 100,000 places located in a 10 km by 10 km square. \nFor a given set of coordinates, your task is to return a ranked list of the most likely places. \nData was fabricated to resemble location signals coming from mobile devices, giving you a flavor of what it takes to work with real data complicated by inaccurate and noisy values. Inconsistent and erroneous location data can disrupt experience for services like Facebook Check In.\n\nRather than ask, “How do I model this artificial world?” let’s ask a different question, “How were these data generated?”\nTreating this as an analysis problem, rather than a modeling problem,\ncould lead to deeper insights than tuning model hyperparameters.\nPlus, it's a fun challenge. "
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats    \nimport scipy.special as sps\n%matplotlib inline"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "df = pd.read_csv('../input/train.csv', index_col=0)\nprint(df.describe())"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "df.head()"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "# Tour of the variables\nLet's pause and see what we're dealing with. \n\n## row_id\nThe primary key for our data. Nothing to see here...\n\n## x, y\nWe're told this is a 10 km x 10 km square and, indeed, x and y run between 0 and 10 so it’s probably fair to assume the units of x and y are kilometers. \nThe precision of x and y is 0.0001 km which is 10 cm (that’s 4 in in Menlo Park).\n\n## accuracy\nThis is interesting. The accuracy varies between 1-1033 with an average of 82. \nIt's given as an integer which is odd. \n\n## time\nAgain an integer. Based on previous analyses, we think the units of this are minutes. \n\n## place_id\n\n"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": ""
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": ""
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "Let's take a look at the number of duplicates."
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "print(df.place_id.drop_duplicates().count())"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "y = df.place_id.value_counts().hist(bins=500)\ny.set(ylabel=\"Duplicate frequency\", xlabel=\"Number of duplicates\")"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "shape, scale = 1.34, 199.38 # mean and dispersion\ns = df.place_id.value_counts().values\n#s = np.random.gamma(shape, scale, 1000)\n\n#Display the histogram of the samples, along with the probability density function:\n\n\ncount, bins, ignored = plt.hist(s, 50, normed=True)\ny = bins**(shape-1)*(np.exp(-bins/scale) /\n                      (sps.gamma(shape)*scale**shape))\n \nplt.plot(bins, y, linewidth=2, color='r')\nplt.ylim(0,.006)\nplt.show()"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": ""
 }
],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}}, "nbformat": 4, "nbformat_minor": 0}